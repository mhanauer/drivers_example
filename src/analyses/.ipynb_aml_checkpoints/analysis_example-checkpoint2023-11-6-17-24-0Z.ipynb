{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%load_ext lab_black"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Number of samples\n",
        "n_samples = 10000\n",
        "\n",
        "# Generate predictors\n",
        "age_60_plus = np.random.binomial(1, 0.3, n_samples)\n",
        "high_cholesterol = np.random.binomial(1, 0.4, n_samples)\n",
        "diabetes = np.random.binomial(1, 0.25, n_samples)\n",
        "preventative_services = np.random.binomial(1, 0.5, n_samples)\n",
        "\n",
        "# Generate hospital_id (from 1 to 10)\n",
        "hospital_id = np.random.randint(1, 4, n_samples)\n",
        "\n",
        "# Initialize the outcome array\n",
        "er_visit = np.zeros(n_samples, dtype=int)\n",
        "\n",
        "# Simulate the outcome based on correlations\n",
        "for i in range(n_samples):\n",
        "    prob = 0.1  # Base probability of ER visit\n",
        "    if age_60_plus[i] == 1:\n",
        "        prob += 0.4\n",
        "    if high_cholesterol[i] == 1:\n",
        "        prob += 0.4\n",
        "    if diabetes[i] == 1:\n",
        "        prob += 0.2\n",
        "    if preventative_services[i] == 1:\n",
        "        prob -= 0.4\n",
        "\n",
        "    er_visit[i] = np.random.binomial(1, min(max(prob, 0), 1))\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame(\n",
        "    {\n",
        "        \"Hospital ID\": hospital_id,\n",
        "        \"Age 60+\": age_60_plus,\n",
        "        \"High Cholesterol\": high_cholesterol,\n",
        "        \"Diabetes\": diabetes,\n",
        "        \"Preventative Services\": preventative_services,\n",
        "        \"ER Visit\": er_visit,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "data[\"Hospital ID\"].value_counts()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "1    3360\n2    3343\n3    3297\nName: Hospital ID, dtype: int64"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1701883335182
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shap\n",
        "\n",
        "# Prepare the data\n",
        "X = data.drop([\"ER Visit\", \"Hospital ID\"], axis=1)\n",
        "y = data[\"ER Visit\"]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train the XGBoost model\n",
        "model = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions (opxtional, to evaluate model)\n",
        "predictions = model.predict(X_test)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701883341942
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shap\n",
        "\n",
        "# Assuming 'data' is your DataFrame and is already defined\n",
        "# ... [Your existing code for data preparation and model training] ...\n",
        "\n",
        "# Create a Tree explainer\n",
        "explainer = shap.Explainer(\n",
        "    model, X_train, model_output=\"probability\", feature_perturbation=\"interventional\"\n",
        ")\n",
        "\n",
        "# Calculate SHAP values - this might take some time for larger datasets\n",
        "shap_values = explainer(data.drop([\"ER Visit\", \"Hospital ID\"], axis=1))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": " 92%|==================  | 9243/10000 [00:17<00:01]       \r 98%|===================| 9822/10000 [00:18<00:00]       "
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701883362310
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming shap_values and X have the same order\n",
        "column_mapping = {\n",
        "    i: name\n",
        "    for i, name in enumerate(data.drop([\"ER Visit\", \"Hospital ID\"], axis=1).columns)\n",
        "}\n",
        "\n",
        "# Rename columns in the DataFrame created from SHAP values\n",
        "data_shap_pd = pd.DataFrame(shap_values.values).rename(columns=column_mapping)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701883373987
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hospital_pd = pd.DataFrame(hospital_id).rename(columns={0: \"Hosptial ID\"})"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701883377062
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_no_id_outcome = data.drop(columns=[\"Hospital ID\", \"ER Visit\"])\n",
        "data_percentile = data_no_id_outcome.rank(pct=True)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701883379058
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step: Add back hosptial ids and add unique ids"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_percentile_id = pd.concat([hospital_pd, data_percentile], axis=1)\n",
        "data_shap_id = pd.concat([hospital_pd, data_shap_pd], axis=1)"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701883380938
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def median_shap_for_high_percentiles(percentile_df, shap_df, id_col):\n",
        "    # Initialize a dictionary to store the results\n",
        "    median_shap_values = {id_col: [], \"variable\": [], \"median_shap\": []}\n",
        "\n",
        "    # Iterate over each ID\n",
        "    for id_value in percentile_df[id_col].unique():\n",
        "        # Filter DataFrames for the current ID\n",
        "        percentile_subdf = percentile_df[percentile_df[id_col] == id_value]\n",
        "        shap_subdf = shap_df[shap_df[id_col] == id_value]\n",
        "\n",
        "        # Iterate over each column (except the ID column)\n",
        "        for col in percentile_df.columns:\n",
        "            if col == id_col:\n",
        "                continue\n",
        "\n",
        "            # Calculate the median of the current column in percentile DataFrame\n",
        "            median_value = percentile_subdf[col].median()\n",
        "\n",
        "            # Filter rows where the percentile value is above the median\n",
        "            rows_above_median = percentile_subdf[\n",
        "                percentile_subdf[col] > median_value\n",
        "            ].index\n",
        "\n",
        "            # Calculate the median SHAP value for these rows\n",
        "            median_shap = shap_subdf.loc[rows_above_median, col].median()\n",
        "\n",
        "            # Store the results\n",
        "            median_shap_values[id_col].append(id_value)\n",
        "            median_shap_values[\"variable\"].append(col)\n",
        "            median_shap_values[\"median_shap\"].append(median_shap)\n",
        "\n",
        "    # Convert the dictionary to a DataFrame and return\n",
        "    return pd.DataFrame(median_shap_values)"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701883384087
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_percentile_id[data_percentile_id[\"Hosptial ID\"] == 2][\"Preventative Services\"]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "5       0.75005\n7       0.25005\n8       0.75005\n9       0.25005\n13      0.75005\n         ...   \n9989    0.75005\n9990    0.25005\n9991    0.75005\n9995    0.75005\n9998    0.25005\nName: Preventative Services, Length: 3343, dtype: float64"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701883389152
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "result_df = (\n",
        "    median_shap_for_high_percentiles(\n",
        "        percentile_df=data_percentile_id, shap_df=data_shap_id, id_col=\"Hosptial ID\"\n",
        "    )\n",
        "    .fillna(-0.20)\n",
        "    .round(2)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701883393255
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Function to add noise\n",
        "def add_noise(series, noise_level=0.05):\n",
        "    # Generate random noise\n",
        "    noise = np.random.uniform(-noise_level, noise_level, series.shape)\n",
        "    return series + noise\n",
        "\n",
        "# Adding noise to each feature\n",
        "result_df['Age 60+'] = add_noise(data['Age 60+'])\n",
        "result_df['High Cholesterol'] = add_noise(result_df['High Cholesterol'])\n",
        "result_df['Diabetes'] = add_noise(data['Diabetes'])\n",
        "result_df['Preventative Services'] = add_noise(result_df['Preventative Services'])\n",
        "\n",
        "# Note: You might want to ensure that the noisy features still make sense\n",
        "# For example, binary features should still be 0 or 1 after adding noise\n",
        "# In such cases, you can round the values or clip them to the [0, 1] range\n",
        "\n",
        "# Display the first few rows of the dataset with noise\n",
        "print(result_df.head())\n"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'High Cholesterol'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/pandas/core/indexes/base.py:2898\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32mpandas/_libs/index.pyx:70\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/index.pyx:101\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:1675\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:1683\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'High Cholesterol'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Adding noise to each feature\u001b[39;00m\n\u001b[1;32m      8\u001b[0m result_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge 60+\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m add_noise(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge 60+\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 9\u001b[0m result_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHigh Cholesterol\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m add_noise(\u001b[43mresult_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHigh Cholesterol\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     10\u001b[0m result_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiabetes\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m add_noise(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiabetes\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     11\u001b[0m result_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreventative Services\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m add_noise(result_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreventative Services\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/pandas/core/frame.py:2906\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 2906\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2907\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   2908\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/pandas/core/indexes/base.py:2900\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   2899\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 2900\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   2902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tolerance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2903\u001b[0m     tolerance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_tolerance(tolerance, np\u001b[38;5;241m.\u001b[39masarray(key))\n",
            "\u001b[0;31mKeyError\u001b[0m: 'High Cholesterol'"
          ]
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1701883399281
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}